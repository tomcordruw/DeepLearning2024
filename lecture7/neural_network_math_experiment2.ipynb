{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu function\n",
    "def activation_ReLu(value):\n",
    "    if value > 0:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# partial derivative of ReLu for the backpropagation\n",
    "def activation_ReLu_part_deriv(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from .csv file\n",
    "df = pd.read_csv(\"medical_insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NN can only handle two feature variables, so in this case we take age and bmi\n",
    "# to predict the target, which is the medical costs/charges for each patient\n",
    "df = df[[\"age\", \"bmi\", \"charges\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 0.36914097444094185\n",
      "Epoch: 2, loss = 0.03815423834419287\n",
      "Epoch: 3, loss = 0.03815315968798274\n",
      "Epoch: 4, loss = 0.03815208165887521\n",
      "Epoch: 5, loss = 0.038151004256451855\n",
      "Epoch: 6, loss = 0.03814992748029375\n",
      "Epoch: 7, loss = 0.038148851329982775\n",
      "Epoch: 8, loss = 0.03814777580510101\n",
      "Epoch: 9, loss = 0.03814670090523111\n",
      "Epoch: 10, loss = 0.038145626629955603\n",
      "Epoch: 11, loss = 0.03814455297885768\n",
      "Epoch: 12, loss = 0.03814347995152063\n",
      "Epoch: 13, loss = 0.038142407547528255\n",
      "Epoch: 14, loss = 0.03814133576646465\n",
      "Epoch: 15, loss = 0.038140264607914386\n",
      "Epoch: 16, loss = 0.038139194071461685\n",
      "Epoch: 17, loss = 0.03813812415669191\n",
      "Epoch: 18, loss = 0.03813705486319047\n",
      "Epoch: 19, loss = 0.03813598619054297\n",
      "Epoch: 20, loss = 0.03813491813833542\n",
      "Epoch: 21, loss = 0.038133850706154014\n",
      "Epoch: 22, loss = 0.03813278389358551\n",
      "Epoch: 23, loss = 0.03813171770021711\n",
      "Epoch: 24, loss = 0.0381306521256358\n",
      "Epoch: 25, loss = 0.03812958716942939\n",
      "Epoch: 26, loss = 0.038128522831185556\n",
      "Epoch: 27, loss = 0.038127459110493\n",
      "Epoch: 28, loss = 0.038126396006939976\n",
      "Epoch: 29, loss = 0.03812533352011559\n",
      "Epoch: 30, loss = 0.0381242716496089\n",
      "Epoch: 31, loss = 0.03812321039500967\n",
      "Epoch: 32, loss = 0.03812214975590744\n",
      "Epoch: 33, loss = 0.03812108973189275\n",
      "Epoch: 34, loss = 0.038120030322555856\n",
      "Epoch: 35, loss = 0.03811897152748785\n",
      "Epoch: 36, loss = 0.03811791334627976\n",
      "Epoch: 37, loss = 0.03811685577852284\n",
      "Epoch: 38, loss = 0.038115798823809256\n",
      "Epoch: 39, loss = 0.03811474248173071\n",
      "Epoch: 40, loss = 0.0381136867518799\n",
      "Epoch: 41, loss = 0.03811263163384956\n",
      "Epoch: 42, loss = 0.03811157712723265\n",
      "Epoch: 43, loss = 0.03811052323162253\n",
      "Epoch: 44, loss = 0.03810946994661304\n",
      "Epoch: 45, loss = 0.03810841727179797\n",
      "Epoch: 46, loss = 0.03810736520677181\n",
      "Epoch: 47, loss = 0.03810631375112928\n",
      "Epoch: 48, loss = 0.03810526290446489\n",
      "Epoch: 49, loss = 0.03810421266637438\n",
      "Epoch: 50, loss = 0.03810316303645312\n",
      "Epoch: 51, loss = 0.0381021140142971\n",
      "Epoch: 52, loss = 0.03810106559950245\n",
      "Epoch: 53, loss = 0.0381000177916657\n",
      "Epoch: 54, loss = 0.03809897059038373\n",
      "Epoch: 55, loss = 0.038097923995253746\n",
      "Epoch: 56, loss = 0.03809687800587296\n",
      "Epoch: 57, loss = 0.03809583262183952\n",
      "Epoch: 58, loss = 0.03809478784275125\n",
      "Epoch: 59, loss = 0.0380937436682065\n",
      "Epoch: 60, loss = 0.03809270009780437\n",
      "Epoch: 61, loss = 0.038091657131143375\n",
      "Epoch: 62, loss = 0.03809061476782327\n",
      "Epoch: 63, loss = 0.03808957300744359\n",
      "Epoch: 64, loss = 0.03808853184960401\n",
      "Epoch: 65, loss = 0.038087491293905124\n",
      "Epoch: 66, loss = 0.03808645133994731\n",
      "Epoch: 67, loss = 0.03808541198733162\n",
      "Epoch: 68, loss = 0.03808437323565924\n",
      "Epoch: 69, loss = 0.038083335084531406\n",
      "Epoch: 70, loss = 0.03808229753355026\n",
      "Epoch: 71, loss = 0.03808126058231783\n",
      "Epoch: 72, loss = 0.03808022423043638\n",
      "Epoch: 73, loss = 0.03807918847750883\n",
      "Epoch: 74, loss = 0.0380781533231383\n",
      "Epoch: 75, loss = 0.03807711876692783\n",
      "Epoch: 76, loss = 0.03807608480848138\n",
      "Epoch: 77, loss = 0.03807505144740278\n",
      "Epoch: 78, loss = 0.03807401868329639\n",
      "Epoch: 79, loss = 0.03807298651576662\n",
      "Epoch: 80, loss = 0.03807195494441841\n",
      "Epoch: 81, loss = 0.03807092396885702\n",
      "Epoch: 82, loss = 0.038069893588687884\n",
      "Epoch: 83, loss = 0.03806886380351709\n",
      "Epoch: 84, loss = 0.03806783461295031\n",
      "Epoch: 85, loss = 0.03806680601659416\n",
      "Epoch: 86, loss = 0.03806577801405528\n",
      "Epoch: 87, loss = 0.03806475060494073\n",
      "Epoch: 88, loss = 0.03806372378885789\n",
      "Epoch: 89, loss = 0.038062697565414326\n",
      "Epoch: 90, loss = 0.03806167193421799\n",
      "Epoch: 91, loss = 0.03806064689487708\n",
      "Epoch: 92, loss = 0.03805962244700001\n",
      "Epoch: 93, loss = 0.03805859859019587\n",
      "Epoch: 94, loss = 0.03805757532407362\n",
      "Epoch: 95, loss = 0.03805655264824272\n",
      "Epoch: 96, loss = 0.0380555305623131\n",
      "Epoch: 97, loss = 0.03805450906589444\n",
      "Epoch: 98, loss = 0.03805348815859742\n",
      "Epoch: 99, loss = 0.03805246784003252\n",
      "Epoch: 100, loss = 0.03805144810981064\n",
      "Epoch: 101, loss = 0.038050428967543105\n",
      "Epoch: 102, loss = 0.03804941041284139\n",
      "Epoch: 103, loss = 0.03804839244531744\n",
      "Epoch: 104, loss = 0.038047375064583104\n",
      "Epoch: 105, loss = 0.03804635827025125\n",
      "Epoch: 106, loss = 0.03804534206193427\n",
      "Epoch: 107, loss = 0.03804432643924551\n",
      "Epoch: 108, loss = 0.038043311401797904\n",
      "Epoch: 109, loss = 0.03804229694920546\n",
      "Epoch: 110, loss = 0.038041283081081824\n",
      "Epoch: 111, loss = 0.0380402697970412\n",
      "Epoch: 112, loss = 0.038039257096698566\n",
      "Epoch: 113, loss = 0.03803824497966824\n",
      "Epoch: 114, loss = 0.038037233445565484\n",
      "Epoch: 115, loss = 0.038036222494005754\n",
      "Epoch: 116, loss = 0.03803521212460476\n",
      "Epoch: 117, loss = 0.038034202336978615\n",
      "Epoch: 118, loss = 0.038033193130743326\n",
      "Epoch: 119, loss = 0.038032184505515605\n",
      "Epoch: 120, loss = 0.03803117646091223\n",
      "Epoch: 121, loss = 0.038030168996550796\n",
      "Epoch: 122, loss = 0.03802916211204826\n",
      "Epoch: 123, loss = 0.03802815580702265\n",
      "Epoch: 124, loss = 0.03802715008109203\n",
      "Epoch: 125, loss = 0.03802614493387462\n",
      "Epoch: 126, loss = 0.038025140364989336\n",
      "Epoch: 127, loss = 0.0380241363740548\n",
      "Epoch: 128, loss = 0.038023132960690396\n",
      "Epoch: 129, loss = 0.03802213012451572\n",
      "Epoch: 130, loss = 0.038021127865150384\n",
      "Epoch: 131, loss = 0.038020126182214585\n",
      "Epoch: 132, loss = 0.03801912507532888\n",
      "Epoch: 133, loss = 0.03801812454411359\n",
      "Epoch: 134, loss = 0.03801712458819007\n",
      "Epoch: 135, loss = 0.038016125207179355\n",
      "Epoch: 136, loss = 0.03801512640070318\n",
      "Epoch: 137, loss = 0.038014128168383184\n",
      "Epoch: 138, loss = 0.03801313050984169\n",
      "Epoch: 139, loss = 0.038012133424701104\n",
      "Epoch: 140, loss = 0.03801113691258408\n",
      "Epoch: 141, loss = 0.038010140973113477\n",
      "Epoch: 142, loss = 0.03800914560591282\n",
      "Epoch: 143, loss = 0.038008150810605636\n",
      "Epoch: 144, loss = 0.03800715658681576\n",
      "Epoch: 145, loss = 0.03800616293416732\n",
      "Epoch: 146, loss = 0.0380051698522848\n",
      "Epoch: 147, loss = 0.03800417734079308\n",
      "Epoch: 148, loss = 0.03800318539931696\n",
      "Epoch: 149, loss = 0.038002194027481735\n",
      "Epoch: 150, loss = 0.03800120322491334\n",
      "Epoch: 151, loss = 0.03800021299123711\n",
      "Epoch: 152, loss = 0.03799922332607976\n",
      "Epoch: 153, loss = 0.03799823422906736\n",
      "Epoch: 154, loss = 0.03799724569982678\n",
      "Epoch: 155, loss = 0.03799625773798493\n",
      "Epoch: 156, loss = 0.037995270343169366\n",
      "Epoch: 157, loss = 0.03799428351500741\n",
      "Epoch: 158, loss = 0.037993297253127\n",
      "Epoch: 159, loss = 0.037992311557156475\n",
      "Epoch: 160, loss = 0.03799132642672414\n",
      "Epoch: 161, loss = 0.03799034186145856\n",
      "Epoch: 162, loss = 0.037989357860989094\n",
      "Epoch: 163, loss = 0.03798837442494482\n",
      "Epoch: 164, loss = 0.03798739155295527\n",
      "Epoch: 165, loss = 0.037986409244650444\n",
      "Epoch: 166, loss = 0.03798542749966035\n",
      "Epoch: 167, loss = 0.03798444631761545\n",
      "Epoch: 168, loss = 0.03798346569814649\n",
      "Epoch: 169, loss = 0.037982485640884356\n",
      "Epoch: 170, loss = 0.03798150614546058\n",
      "Epoch: 171, loss = 0.037980527211506424\n",
      "Epoch: 172, loss = 0.03797954883865378\n",
      "Epoch: 173, loss = 0.03797857102653472\n",
      "Epoch: 174, loss = 0.03797759377478166\n",
      "Epoch: 175, loss = 0.03797661708302752\n",
      "Epoch: 176, loss = 0.03797564095090486\n",
      "Epoch: 177, loss = 0.03797466537804711\n",
      "Epoch: 178, loss = 0.0379736903640876\n",
      "Epoch: 179, loss = 0.03797271590866046\n",
      "Epoch: 180, loss = 0.037971742011399336\n",
      "Epoch: 181, loss = 0.03797076867193904\n",
      "Epoch: 182, loss = 0.03796979588991373\n",
      "Epoch: 183, loss = 0.03796882366495843\n",
      "Epoch: 184, loss = 0.03796785199670831\n",
      "Epoch: 185, loss = 0.03796688088479919\n",
      "Epoch: 186, loss = 0.03796591032886637\n",
      "Epoch: 187, loss = 0.037964940328545846\n",
      "Epoch: 188, loss = 0.03796397088347417\n",
      "Epoch: 189, loss = 0.037963001993287906\n",
      "Epoch: 190, loss = 0.037962033657623476\n",
      "Epoch: 191, loss = 0.037961065876118354\n",
      "Epoch: 192, loss = 0.037960098648409754\n",
      "Epoch: 193, loss = 0.03795913197413542\n",
      "Epoch: 194, loss = 0.037958165852933205\n",
      "Epoch: 195, loss = 0.03795720028444164\n",
      "Epoch: 196, loss = 0.03795623526829879\n",
      "Epoch: 197, loss = 0.03795527080414367\n",
      "Epoch: 198, loss = 0.03795430689161527\n",
      "Epoch: 199, loss = 0.037953343530352825\n",
      "Epoch: 200, loss = 0.037952380719995765\n",
      "Epoch: 201, loss = 0.03795141846018435\n",
      "Epoch: 202, loss = 0.03795045675055846\n",
      "Epoch: 203, loss = 0.03794949559075821\n",
      "Epoch: 204, loss = 0.03794853498042491\n",
      "Epoch: 205, loss = 0.03794757491919906\n",
      "Epoch: 206, loss = 0.03794661540672201\n",
      "Epoch: 207, loss = 0.03794565644263516\n",
      "Epoch: 208, loss = 0.03794469802658036\n",
      "Epoch: 209, loss = 0.03794374015819966\n",
      "Epoch: 210, loss = 0.0379427828371351\n",
      "Epoch: 211, loss = 0.037941826063029614\n",
      "Epoch: 212, loss = 0.037940869835525845\n",
      "Epoch: 213, loss = 0.03793991415426704\n",
      "Epoch: 214, loss = 0.03793895901889652\n",
      "Epoch: 215, loss = 0.03793800442905764\n",
      "Epoch: 216, loss = 0.03793705038439473\n",
      "Epoch: 217, loss = 0.03793609688455184\n",
      "Epoch: 218, loss = 0.0379351439291735\n",
      "Epoch: 219, loss = 0.037934191517904337\n",
      "Epoch: 220, loss = 0.037933239650389264\n",
      "Epoch: 221, loss = 0.03793228832627385\n",
      "Epoch: 222, loss = 0.03793133754520326\n",
      "Epoch: 223, loss = 0.0379303873068235\n",
      "Epoch: 224, loss = 0.03792943761078085\n",
      "Epoch: 225, loss = 0.03792848845672107\n",
      "Epoch: 226, loss = 0.03792753984429137\n",
      "Epoch: 227, loss = 0.03792659177313822\n",
      "Epoch: 228, loss = 0.03792564424290899\n",
      "Epoch: 229, loss = 0.03792469725325097\n",
      "Epoch: 230, loss = 0.037923750803811655\n",
      "Epoch: 231, loss = 0.03792280489423956\n",
      "Epoch: 232, loss = 0.0379218595241822\n",
      "Epoch: 233, loss = 0.037920914693288364\n",
      "Epoch: 234, loss = 0.03791997040120669\n",
      "Epoch: 235, loss = 0.0379190266475864\n",
      "Epoch: 236, loss = 0.03791808343207644\n",
      "Epoch: 237, loss = 0.0379171407543267\n",
      "Epoch: 238, loss = 0.03791619861398663\n",
      "Epoch: 239, loss = 0.037915257010706416\n",
      "Epoch: 240, loss = 0.03791431594413626\n",
      "Epoch: 241, loss = 0.03791337541392699\n",
      "Epoch: 242, loss = 0.037912435419729204\n",
      "Epoch: 243, loss = 0.037911495961194086\n",
      "Epoch: 244, loss = 0.03791055703797318\n",
      "Epoch: 245, loss = 0.03790961864971798\n",
      "Epoch: 246, loss = 0.0379086807960802\n",
      "Epoch: 247, loss = 0.037907743476712316\n",
      "Epoch: 248, loss = 0.03790680669126663\n",
      "Epoch: 249, loss = 0.03790587043939566\n",
      "Epoch: 250, loss = 0.03790493472075261\n",
      "Epoch: 251, loss = 0.03790399953499044\n",
      "Epoch: 252, loss = 0.037903064881762774\n",
      "Epoch: 253, loss = 0.03790213076072333\n",
      "Epoch: 254, loss = 0.037901197171526076\n",
      "Epoch: 255, loss = 0.037900264113824925\n",
      "Epoch: 256, loss = 0.03789933158727512\n",
      "Epoch: 257, loss = 0.037898399591530706\n",
      "Epoch: 258, loss = 0.03789746812624709\n",
      "Epoch: 259, loss = 0.03789653719107938\n",
      "Epoch: 260, loss = 0.037895606785683225\n",
      "Epoch: 261, loss = 0.037894676909714375\n",
      "Epoch: 262, loss = 0.03789374756282885\n",
      "Epoch: 263, loss = 0.03789281874468313\n",
      "Epoch: 264, loss = 0.03789189045493359\n",
      "Epoch: 265, loss = 0.03789096269323721\n",
      "Epoch: 266, loss = 0.03789003545925094\n",
      "Epoch: 267, loss = 0.03788910875263228\n",
      "Epoch: 268, loss = 0.037888182573038685\n",
      "Epoch: 269, loss = 0.037887256920128225\n",
      "Epoch: 270, loss = 0.037886331793558714\n",
      "Epoch: 271, loss = 0.0378854071929887\n",
      "Epoch: 272, loss = 0.0378844831180771\n",
      "Epoch: 273, loss = 0.03788355956848223\n",
      "Epoch: 274, loss = 0.03788263654386354\n",
      "Epoch: 275, loss = 0.037881714043880466\n",
      "Epoch: 276, loss = 0.037880792068192524\n",
      "Epoch: 277, loss = 0.03787987061645979\n",
      "Epoch: 278, loss = 0.03787894968834227\n",
      "Epoch: 279, loss = 0.03787802928350048\n",
      "Epoch: 280, loss = 0.03787710940159495\n",
      "Epoch: 281, loss = 0.03787619004228684\n",
      "Epoch: 282, loss = 0.03787527120523721\n",
      "Epoch: 283, loss = 0.037874352890107446\n",
      "Epoch: 284, loss = 0.03787343509655916\n",
      "Epoch: 285, loss = 0.0378725178242546\n",
      "Epoch: 286, loss = 0.03787160107285554\n",
      "Epoch: 287, loss = 0.03787068484202479\n",
      "Epoch: 288, loss = 0.03786976913142496\n",
      "Epoch: 289, loss = 0.03786885394071887\n",
      "Epoch: 290, loss = 0.037867939269569946\n",
      "Epoch: 291, loss = 0.037867025117641284\n",
      "Epoch: 292, loss = 0.037866111484596884\n",
      "Epoch: 293, loss = 0.03786519837010054\n",
      "Epoch: 294, loss = 0.03786428577381664\n",
      "Epoch: 295, loss = 0.03786337369540962\n",
      "Epoch: 296, loss = 0.037862462134543945\n",
      "Epoch: 297, loss = 0.037861551090884724\n",
      "Epoch: 298, loss = 0.037860640564097416\n",
      "Epoch: 299, loss = 0.03785973055384728\n",
      "Epoch: 300, loss = 0.03785882105979977\n",
      "--------------------------\n",
      "ORIGINAL WEIGHTS/BIASES:\n",
      "\n",
      "W1: -7.27\n",
      "W2: 7.66\n",
      "W3: -7.06\n",
      "W4: -3.0\n",
      "W5: -3.47\n",
      "W6: 9.33\n",
      "B1: -2.13\n",
      "B2: 2.12\n",
      "B3: -2.4\n",
      "--------------------------\n",
      "FINAL WEIGHTS/BIASES:\n",
      "\n",
      "W1: -7.27\n",
      "W2: 7.793708960302096\n",
      "W3: -7.06\n",
      "W4: -2.9419645401691463\n",
      "W5: -3.47\n",
      "W6: 8.614395035214784\n",
      "B1: -2.13\n",
      "B2: 0.3251514794696843\n",
      "B3: -2.5930837241427023\n"
     ]
    }
   ],
   "source": [
    "# Initialising weights and biases\n",
    "# Randomizing weights to be floats between -10 and 10, rounded to two decimals\n",
    "\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "for i in range(6):\n",
    "    weight = round(random.uniform(-10,10), 2)\n",
    "    weights.append(weight)\n",
    "    \n",
    "w1 = weights[0]\n",
    "w2 = weights[1]\n",
    "w3 = weights[2]\n",
    "w4 = weights[3]\n",
    "w5 = weights[4]\n",
    "w6 = weights[5]\n",
    "\n",
    "# Biases\n",
    "for i in range(3):\n",
    "    bias = round(random.uniform(-5,5), 2)\n",
    "    biases.append(bias)\n",
    "\n",
    "bias1 = biases[0]\n",
    "bias2 = biases[1]\n",
    "bias3 = biases[2]\n",
    "\n",
    "# save the original weights and biases for comparison in the end\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# learning rate\n",
    "LR = 0.001\n",
    "epochs = 300\n",
    "\n",
    "# Using the previously defined function to generate data instead\n",
    "data = list(df.values)\n",
    "\n",
    "# Let's scale our values with min/max -scaling\n",
    "data=(data - np.min(data))/(np.max(data)-np.min(data))\n",
    "\n",
    "# Points for plotting loss later\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "        # Forward pass\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        \n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        predicted_value = node_3_output\n",
    "        # This will probably crash if the loss value gets too high\n",
    "        # replace with Numpy64 if needed\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # Back propagation - last layer\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # Back propagation - next layer\n",
    "        # From this point the chain rule is needed\n",
    "\n",
    "        # Weights 1 to 4:\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # Biases 1 and 2:\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # Updating the weights and biases\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    # after each epoch, print the current weights and biases\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss = {average_loss}\")\n",
    "\n",
    "print(\"--------------------------\")\n",
    "print(\"ORIGINAL WEIGHTS/BIASES:\\n\")\n",
    "print(f\"W1: {original_w1}\")\n",
    "print(f\"W2: {original_w2}\")\n",
    "print(f\"W3: {original_w3}\")\n",
    "print(f\"W4: {original_w4}\")\n",
    "print(f\"W5: {original_w5}\")\n",
    "print(f\"W6: {original_w6}\")\n",
    "print(f\"B1: {original_b1}\")\n",
    "print(f\"B2: {original_b2}\")\n",
    "print(f\"B3: {original_b3}\")\n",
    "\n",
    "# IDEA: have a small amount of epochs\n",
    "# but print the current weights after each epoch\n",
    "# can you see a certain development in certain\n",
    "# weights and biases? discuss.\n",
    "print(\"--------------------------\")\n",
    "print(\"FINAL WEIGHTS/BIASES:\\n\")\n",
    "print(f\"W1: {w1}\")\n",
    "print(f\"W2: {w2}\")\n",
    "print(f\"W3: {w3}\")\n",
    "print(f\"W4: {w4}\")\n",
    "print(f\"W5: {w5}\")\n",
    "print(f\"W6: {w6}\")\n",
    "print(f\"B1: {bias1}\")\n",
    "print(f\"B2: {bias2}\")\n",
    "print(f\"B3: {bias3}\")\n",
    "\n",
    "# sometimes, if starting weights and biases\n",
    "# are large and completely far from each other\n",
    "# the neural network seems to emphasize\n",
    "# only one side of the neural network\n",
    "# this implies a question, does e.g. TensorFlow\n",
    "# alternate the order of weight calculates from epoch to epoch\n",
    "# in order to utilize all the weights etc. more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction is basically just doing the forward \n",
    "# pass again (but only that)\n",
    "def predict(x1, x2):\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = x1 * w1 + x2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "    node_1_output\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = x1 * w2 + x2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "    node_2_output\n",
    "\n",
    "    # NODE 3 OUTPUT \n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    node_3_output\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Since we scaled everything, testing the values is a bit tricky</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.9</td>\n",
       "      <td>16884.924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   bmi    charges\n",
       "0   19  27.9  16884.924"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a test row in original values:\n",
    "df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.76829326e-05, 1.87280992e-04, 2.64592656e-01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # get the same row in scaled version\n",
    "# you can google for decimal formats, for example:\n",
    "# \"4.76829326e-05 in decimal format\"\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = 0.00004768293 (19 years of age)\n",
    "# x2 = 0.00018728099 (27.9 BMI)\n",
    "# y/target = 0.264592656 (16884.924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20635459907074116"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output with scaled values is this\n",
    "# if you multiply this with the MAX value of charges, \n",
    "# you get USD value\n",
    "predict(0.00004768293, 0.00018728099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13159.321104573113"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # get the USD value\n",
    "df['charges'].max() * predict(0.00004768293, 0.00018728099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113048</td>\n",
       "      <td>0.298624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>0.113048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charges</th>\n",
       "      <td>0.298624</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age       bmi   charges\n",
       "age      1.000000  0.113048  0.298624\n",
       "bmi      0.113048  1.000000  0.199846\n",
       "charges  0.298624  0.199846  1.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
