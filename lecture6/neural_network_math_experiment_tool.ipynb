{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ReLu function\n",
    "def activation_ReLu(value):\n",
    "    if value > 0:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# partial derivative of ReLu for the backpropagation\n",
    "def activation_ReLu_part_deriv(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_test_data():\n",
    "    result = []\n",
    "\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "        n3= n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Epoch: 1, loss = 86.8219701491658\n",
      "----------------\n",
      "Epoch: 2, loss = 35.05662380168375\n",
      "----------------\n",
      "Epoch: 3, loss = 34.01338804469109\n",
      "----------------\n",
      "Epoch: 4, loss = 33.994016673528606\n",
      "----------------\n",
      "Epoch: 5, loss = 33.993540301562476\n",
      "----------------\n",
      "Epoch: 6, loss = 33.993513930752215\n",
      "----------------\n",
      "Epoch: 7, loss = 33.99351108079899\n",
      "----------------\n",
      "Epoch: 8, loss = 33.993510714224676\n",
      "----------------\n",
      "Epoch: 9, loss = 33.99351066581\n",
      "----------------\n",
      "Epoch: 10, loss = 33.9935106593928\n",
      "----------------\n",
      "Epoch: 11, loss = 33.9935106585418\n",
      "----------------\n",
      "Epoch: 12, loss = 33.99351065842896\n",
      "----------------\n",
      "Epoch: 13, loss = 33.99351065841398\n",
      "----------------\n",
      "Epoch: 14, loss = 33.993510658411985\n",
      "----------------\n",
      "Epoch: 15, loss = 33.993510658411736\n",
      "----------------\n",
      "Epoch: 16, loss = 33.9935106584117\n",
      "----------------\n",
      "Epoch: 17, loss = 33.993510658411694\n",
      "----------------\n",
      "Epoch: 18, loss = 33.9935106584117\n",
      "----------------\n",
      "Epoch: 19, loss = 33.9935106584117\n",
      "----------------\n",
      "Epoch: 20, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 21, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 22, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 23, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 24, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 25, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 26, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 27, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 28, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 29, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 30, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 31, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 32, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 33, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 34, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 35, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 36, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 37, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 38, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 39, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 40, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 41, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 42, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 43, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 44, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 45, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 46, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 47, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 48, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 49, loss = 33.99351065841171\n",
      "----------------\n",
      "Epoch: 50, loss = 33.99351065841171\n"
     ]
    }
   ],
   "source": [
    "# Initialising weights and biases\n",
    "# Weights\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "\n",
    "# Biases\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# save the original weights and biases for comparison in the end\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# input values and target value\n",
    "#input1 = 1\n",
    "#input2 = 0\n",
    "#true_value = 2\n",
    "\n",
    "# learning rate\n",
    "LR = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# our data\n",
    "\"\"\"data = [\n",
    "    [1, 0, 2],\n",
    "    [2, 1, 6],\n",
    "    [3, 3, 17]\n",
    "]\"\"\"\n",
    "\n",
    "# Using the previously defined function to generate data instead\n",
    "data = generate_test_data()\n",
    "\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "        # Forward pass\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        \n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        predicted_value = node_3_output\n",
    "        # This will probably crash if the loss value gets too high\n",
    "        # replace with Numpy64 if needed\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # Back propagation - last layer\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # Back propagation - next layer\n",
    "        # From this point the chain rule is needed\n",
    "\n",
    "        # Weights 1 to 4:\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # Biases 1 and 2:\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # Updating the weights and biases\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    # after each epoch, print the current weights and biases\n",
    "    loss_points.append(average_loss)\n",
    "    \n",
    "    print(\"----------------\")\n",
    "    print(f\"Epoch: {epoch +1}, loss = {average_loss}\")\n",
    "    \"\"\"print(f\"W1: {new_w1}\")\n",
    "    print(f\"W2: {new_w2}\")\n",
    "    print(f\"W3: {new_w3}\")\n",
    "    print(f\"W4: {new_w4}\")\n",
    "    print(f\"W5: {new_w5}\")\n",
    "    print(f\"W6: {new_w6}\")\n",
    "    print(f\"B1: {new_b1}\")\n",
    "    print(f\"B2: {new_b2}\")\n",
    "    print(f\"B3: {new_b3}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Original weights/biases\n",
      "W1: 1\n",
      "W2: 0.5\n",
      "W3: 1\n",
      "W4: -0.5\n",
      "W5: 1\n",
      "W6: 1\n",
      "B1: 0.5\n",
      "B2: 0\n",
      "B3: 0.5\n",
      "-------------------\n",
      "Final weights/biases\n",
      "W1: 2.5742598406171187\n",
      "W2: 0.7594475720019436\n",
      "W3: -9.922388722916562\n",
      "W4: -1.9745198960782555\n",
      "W5: -4.635691921455037\n",
      "W6: -0.4284670480161607\n",
      "B1: -1.2660194157412081\n",
      "B2: -0.19998794074840648\n",
      "B3: 12.334441091141919\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Original weights/biases\")\n",
    "print(f\"W1: {original_w1}\")\n",
    "print(f\"W2: {original_w2}\")\n",
    "print(f\"W3: {original_w3}\")\n",
    "print(f\"W4: {original_w4}\")\n",
    "print(f\"W5: {original_w5}\")\n",
    "print(f\"W6: {original_w6}\")\n",
    "print(f\"B1: {original_b1}\")\n",
    "print(f\"B2: {original_b2}\")\n",
    "print(f\"B3: {original_b3}\")\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Final weights/biases\")\n",
    "print(f\"W1: {w1}\")\n",
    "print(f\"W2: {w2}\")\n",
    "print(f\"W3: {w3}\")\n",
    "print(f\"W4: {w4}\")\n",
    "print(f\"W5: {w5}\")\n",
    "print(f\"W6: {w6}\")\n",
    "print(f\"B1: {bias1}\")\n",
    "print(f\"B2: {bias2}\")\n",
    "print(f\"B3: {bias3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction is just doing the forward pass step\n",
    "def predict(x1, x2):\n",
    "    node_1_output = x1 * w1 + x2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "    \n",
    "    node_2_output = x1 * w2 + x2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.334441091141919"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
