{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu function\n",
    "def activation_ReLu(value):\n",
    "    if value > 0:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# partial derivative of ReLu for the backpropagation\n",
    "def activation_ReLu_part_deriv(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness\n",
    "#np.random.seed(123)\n",
    "\n",
    "def generate_test_data():\n",
    "    result = []\n",
    "\n",
    "    for x in range(300):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "        n3= n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 0.05883470854364731\n",
      "Epoch: 2, loss = 0.029062240345845786\n",
      "Epoch: 3, loss = 0.026726806497678706\n",
      "Epoch: 4, loss = 0.024485599614725233\n",
      "Epoch: 5, loss = 0.022364190716451106\n",
      "Epoch: 6, loss = 0.02036835464108957\n",
      "Epoch: 7, loss = 0.01852614000349715\n",
      "Epoch: 8, loss = 0.01684009041499183\n",
      "Epoch: 9, loss = 0.015322034111473004\n",
      "Epoch: 10, loss = 0.013966139325246592\n",
      "Epoch: 11, loss = 0.012786101594225097\n",
      "Epoch: 12, loss = 0.011765276242180782\n",
      "Epoch: 13, loss = 0.01088578729520248\n",
      "Epoch: 14, loss = 0.010123122790439979\n",
      "Epoch: 15, loss = 0.009471755114022358\n",
      "Epoch: 16, loss = 0.008914130081271506\n",
      "Epoch: 17, loss = 0.008413363130444001\n",
      "Epoch: 18, loss = 0.007980969845669875\n",
      "Epoch: 19, loss = 0.007616203155535944\n",
      "Epoch: 20, loss = 0.0073093241357790936\n",
      "Epoch: 21, loss = 0.007022792170106099\n",
      "Epoch: 22, loss = 0.006747961576672155\n",
      "Epoch: 23, loss = 0.0065107545465835056\n",
      "Epoch: 24, loss = 0.006291556990250886\n",
      "Epoch: 25, loss = 0.006115007682424793\n",
      "Epoch: 26, loss = 0.005962756474401803\n",
      "Epoch: 27, loss = 0.0058259802532199855\n",
      "Epoch: 28, loss = 0.005702780883556665\n",
      "Epoch: 29, loss = 0.005588749346408611\n",
      "Epoch: 30, loss = 0.005487650102063521\n",
      "Epoch: 31, loss = 0.005389986835450557\n",
      "Epoch: 32, loss = 0.005306132689548142\n",
      "Epoch: 33, loss = 0.005227555699086102\n",
      "Epoch: 34, loss = 0.005153595350434607\n",
      "Epoch: 35, loss = 0.005093250332005638\n",
      "Epoch: 36, loss = 0.005032198512002622\n",
      "Epoch: 37, loss = 0.004976695820329547\n",
      "Epoch: 38, loss = 0.004923826654725378\n",
      "Epoch: 39, loss = 0.004882245275806892\n",
      "Epoch: 40, loss = 0.004844022643908599\n",
      "Epoch: 41, loss = 0.004814667566666691\n",
      "Epoch: 42, loss = 0.004780034700246538\n",
      "Epoch: 43, loss = 0.004748259902722479\n",
      "Epoch: 44, loss = 0.004718103151819755\n",
      "Epoch: 45, loss = 0.004687554819774703\n",
      "Epoch: 46, loss = 0.004660897086190089\n",
      "Epoch: 47, loss = 0.004634056338374154\n",
      "Epoch: 48, loss = 0.004607954711102463\n",
      "Epoch: 49, loss = 0.004582638867693202\n",
      "Epoch: 50, loss = 0.004559602040047025\n",
      "Epoch: 51, loss = 0.0045364943413616675\n",
      "Epoch: 52, loss = 0.004513909326130708\n",
      "Epoch: 53, loss = 0.004491914723485538\n",
      "Epoch: 54, loss = 0.004470488798750385\n",
      "Epoch: 55, loss = 0.004449607442929805\n",
      "Epoch: 56, loss = 0.004429247736168255\n",
      "Epoch: 57, loss = 0.004409388007971222\n",
      "Epoch: 58, loss = 0.004390007760794366\n",
      "Epoch: 59, loss = 0.004371087594338141\n",
      "Epoch: 60, loss = 0.00435260913554228\n",
      "Epoch: 61, loss = 0.004334408183386215\n",
      "Epoch: 62, loss = 0.004316923334633431\n",
      "Epoch: 63, loss = 0.004298806922366587\n",
      "Epoch: 64, loss = 0.0042828814542422685\n",
      "Epoch: 65, loss = 0.004266890314173823\n",
      "Epoch: 66, loss = 0.004251077428123509\n",
      "Epoch: 67, loss = 0.00423521538645211\n",
      "Epoch: 68, loss = 0.0042191969009859\n",
      "Epoch: 69, loss = 0.004206731837999574\n",
      "Epoch: 70, loss = 0.004192774201239054\n",
      "Epoch: 71, loss = 0.004178958283949287\n",
      "Epoch: 72, loss = 0.004165311633032319\n",
      "Epoch: 73, loss = 0.00415194566632187\n",
      "Epoch: 74, loss = 0.004138723962291509\n",
      "Epoch: 75, loss = 0.004125684365835512\n",
      "Epoch: 76, loss = 0.004112824539770521\n",
      "Epoch: 77, loss = 0.004099314683341422\n",
      "Epoch: 78, loss = 0.004086650176579729\n",
      "Epoch: 79, loss = 0.0040742223709801075\n",
      "Epoch: 80, loss = 0.004062015042318892\n",
      "Epoch: 81, loss = 0.00404995442030585\n",
      "Epoch: 82, loss = 0.004038050666555221\n",
      "Epoch: 83, loss = 0.0040271571399703885\n",
      "Epoch: 84, loss = 0.004015655799294703\n",
      "Epoch: 85, loss = 0.004004243813564535\n",
      "Epoch: 86, loss = 0.003992975978627473\n",
      "Epoch: 87, loss = 0.00398185169462118\n",
      "Epoch: 88, loss = 0.003970868930647208\n",
      "Epoch: 89, loss = 0.003959208450817774\n",
      "Epoch: 90, loss = 0.003948847024374826\n",
      "Epoch: 91, loss = 0.0039383529634881625\n",
      "Epoch: 92, loss = 0.00392797804887381\n",
      "Epoch: 93, loss = 0.003917326972313065\n",
      "Epoch: 94, loss = 0.003908503051267296\n",
      "Epoch: 95, loss = 0.0038985509861482934\n",
      "Epoch: 96, loss = 0.0038886808341222624\n",
      "Epoch: 97, loss = 0.0038789247811489556\n",
      "Epoch: 98, loss = 0.0038692825094809335\n",
      "Epoch: 99, loss = 0.003859752951602578\n",
      "Epoch: 100, loss = 0.0038503350487825707\n",
      "Epoch: 101, loss = 0.00384102776591465\n",
      "Epoch: 102, loss = 0.003831830089071216\n",
      "Epoch: 103, loss = 0.0038227410229977783\n",
      "Epoch: 104, loss = 0.003813759588912101\n",
      "Epoch: 105, loss = 0.0038048848225790332\n",
      "Epoch: 106, loss = 0.0037961157726278155\n",
      "Epoch: 107, loss = 0.0037874514990820854\n",
      "Epoch: 108, loss = 0.0037788910720770027\n",
      "Epoch: 109, loss = 0.003770433570740758\n",
      "Epoch: 110, loss = 0.00376207808222069\n",
      "Epoch: 111, loss = 0.0037538237008366174\n",
      "Epoch: 112, loss = 0.003745669527346112\n",
      "Epoch: 113, loss = 0.0037376146683083837\n",
      "Epoch: 114, loss = 0.003738391764003231\n",
      "Epoch: 115, loss = 0.003729203248617381\n",
      "Epoch: 116, loss = 0.0037211725107411803\n",
      "Epoch: 117, loss = 0.003713250037466617\n",
      "Epoch: 118, loss = 0.0037054311094234884\n",
      "Epoch: 119, loss = 0.0036977144204476124\n",
      "Epoch: 120, loss = 0.003690098760869133\n",
      "Epoch: 121, loss = 0.0036849070809072183\n",
      "Epoch: 122, loss = 0.00367655980933734\n",
      "Epoch: 123, loss = 0.003668965121340911\n",
      "Epoch: 124, loss = 0.003661488897433175\n",
      "Epoch: 125, loss = 0.0036556981585320813\n",
      "Epoch: 126, loss = 0.003648509129153939\n",
      "Epoch: 127, loss = 0.0036413739369504567\n",
      "Epoch: 128, loss = 0.0036343331855861432\n",
      "Epoch: 129, loss = 0.003627386580691816\n",
      "Epoch: 130, loss = 0.003620533086814979\n",
      "Epoch: 131, loss = 0.003613771669857472\n",
      "Epoch: 132, loss = 0.003607958833923292\n",
      "Epoch: 133, loss = 0.0036012086380828126\n",
      "Epoch: 134, loss = 0.0035970435896760973\n",
      "Epoch: 135, loss = 0.003592339958660454\n",
      "Epoch: 136, loss = 0.00358563658562625\n",
      "Epoch: 137, loss = 0.003578831323751745\n",
      "Epoch: 138, loss = 0.003572586873823759\n",
      "Epoch: 139, loss = 0.003566381665703243\n",
      "Epoch: 140, loss = 0.0035602607795925285\n",
      "Epoch: 141, loss = 0.003554224067826187\n",
      "Epoch: 142, loss = 0.0035482705408683636\n",
      "Epoch: 143, loss = 0.0035423992128650337\n",
      "Epoch: 144, loss = 0.003536609113757386\n",
      "Epoch: 145, loss = 0.0035316868850368073\n",
      "Epoch: 146, loss = 0.003526058975198533\n",
      "Epoch: 147, loss = 0.0035205038614256937\n",
      "Epoch: 148, loss = 0.003515025188160978\n",
      "Epoch: 149, loss = 0.0035096221996173513\n",
      "Epoch: 150, loss = 0.0035042940783670956\n",
      "Epoch: 151, loss = 0.003499040010934095\n",
      "Epoch: 152, loss = 0.003493859188647898\n",
      "Epoch: 153, loss = 0.0034887508075643085\n",
      "Epoch: 154, loss = 0.003483714068390383\n",
      "Epoch: 155, loss = 0.0034787481764258187\n",
      "Epoch: 156, loss = 0.0034738523415188743\n",
      "Epoch: 157, loss = 0.0034690257780354092\n",
      "Epoch: 158, loss = 0.0034642677048393796\n",
      "Epoch: 159, loss = 0.003459577345283567\n",
      "Epoch: 160, loss = 0.00345495392720922\n",
      "Epoch: 161, loss = 0.0034503966829535835\n",
      "Epoch: 162, loss = 0.0034459048493642674\n",
      "Epoch: 163, loss = 0.003440305244390686\n",
      "Epoch: 164, loss = 0.0034358618618077394\n",
      "Epoch: 165, loss = 0.003431437599754168\n",
      "Epoch: 166, loss = 0.0034270776502420925\n",
      "Epoch: 167, loss = 0.0034227818077233645\n",
      "Epoch: 168, loss = 0.00341854924513813\n",
      "Epoch: 169, loss = 0.0034143791412575516\n",
      "Epoch: 170, loss = 0.0034102706881872684\n",
      "Epoch: 171, loss = 0.003406834807304695\n",
      "Epoch: 172, loss = 0.003402449860956362\n",
      "Epoch: 173, loss = 0.003398013691246192\n",
      "Epoch: 174, loss = 0.00339363209255858\n",
      "Epoch: 175, loss = 0.003389319257053697\n",
      "Epoch: 176, loss = 0.0033850739985520875\n",
      "Epoch: 177, loss = 0.0033801359382044953\n",
      "Epoch: 178, loss = 0.0033751852050933227\n",
      "Epoch: 179, loss = 0.0033715575538742076\n",
      "Epoch: 180, loss = 0.0033669986345115613\n",
      "Epoch: 181, loss = 0.0033626258563337883\n",
      "Epoch: 182, loss = 0.0033582736833161026\n",
      "Epoch: 183, loss = 0.003353989322215747\n",
      "Epoch: 184, loss = 0.0033497739454014827\n",
      "Epoch: 185, loss = 0.0033456260721365693\n",
      "Epoch: 186, loss = 0.003341544270127848\n",
      "Epoch: 187, loss = 0.0033375271794995457\n",
      "Epoch: 188, loss = 0.003333573506786898\n",
      "Epoch: 189, loss = 0.003329682019185408\n",
      "Epoch: 190, loss = 0.003325606233366299\n",
      "Epoch: 191, loss = 0.003325599299981172\n",
      "Epoch: 192, loss = 0.003322148511492576\n",
      "Epoch: 193, loss = 0.0033186141821562863\n",
      "Epoch: 194, loss = 0.003315129366974914\n",
      "Epoch: 195, loss = 0.0033116943282751788\n",
      "Epoch: 196, loss = 0.003308312315622045\n",
      "Epoch: 197, loss = 0.003297307161458908\n",
      "Epoch: 198, loss = 0.0032936436797747935\n",
      "Epoch: 199, loss = 0.0032900561792254305\n",
      "Epoch: 200, loss = 0.0032865232024189297\n",
      "Epoch: 201, loss = 0.003283026599653288\n",
      "Epoch: 202, loss = 0.003279705732770593\n",
      "Epoch: 203, loss = 0.0032760092903234435\n",
      "Epoch: 204, loss = 0.0032723305724591395\n",
      "Epoch: 205, loss = 0.0032687021249099483\n",
      "Epoch: 206, loss = 0.0032670132956160165\n",
      "Epoch: 207, loss = 0.0032599718935158875\n",
      "Epoch: 208, loss = 0.0032567320408753674\n",
      "Epoch: 209, loss = 0.003253488826538541\n",
      "Epoch: 210, loss = 0.003250280315946652\n",
      "Epoch: 211, loss = 0.0032471066195694253\n",
      "Epoch: 212, loss = 0.0032440968011355096\n",
      "Epoch: 213, loss = 0.003241209123273867\n",
      "Epoch: 214, loss = 0.0032383291058203506\n",
      "Epoch: 215, loss = 0.003235479504792751\n",
      "Epoch: 216, loss = 0.0032326602384933076\n",
      "Epoch: 217, loss = 0.0032298710084194314\n",
      "Epoch: 218, loss = 0.003227111520741572\n",
      "Epoch: 219, loss = 0.0032243814879302276\n",
      "Epoch: 220, loss = 0.0032216806282915258\n",
      "Epoch: 221, loss = 0.0032190086655246785\n",
      "Epoch: 222, loss = 0.003216365328318105\n",
      "Epoch: 223, loss = 0.0032164949031005556\n",
      "Epoch: 224, loss = 0.0032141581928413748\n",
      "Epoch: 225, loss = 0.00321174333697187\n",
      "Epoch: 226, loss = 0.0032093542621037287\n",
      "Epoch: 227, loss = 0.003206410280837098\n",
      "Epoch: 228, loss = 0.0032040750261368347\n",
      "Epoch: 229, loss = 0.0032017645037222663\n",
      "Epoch: 230, loss = 0.0032000953093848828\n",
      "Epoch: 231, loss = 0.003197862944486226\n",
      "Epoch: 232, loss = 0.003195629189824517\n",
      "Epoch: 233, loss = 0.0031934186205791588\n",
      "Epoch: 234, loss = 0.0031912311176068558\n",
      "Epoch: 235, loss = 0.0031890663548750833\n",
      "Epoch: 236, loss = 0.003186924017938798\n",
      "Epoch: 237, loss = 0.0031848038049064205\n",
      "Epoch: 238, loss = 0.003182705425548452\n",
      "Epoch: 239, loss = 0.0031840334078525037\n",
      "Epoch: 240, loss = 0.0031822034549165777\n",
      "Epoch: 241, loss = 0.00318028700317425\n",
      "Epoch: 242, loss = 0.0031783908504201693\n",
      "Epoch: 243, loss = 0.0031765154769404854\n",
      "Epoch: 244, loss = 0.003174660457066997\n",
      "Epoch: 245, loss = 0.0031728253797955104\n",
      "Epoch: 246, loss = 0.0031710098550877647\n",
      "Epoch: 247, loss = 0.0031692135124716833\n",
      "Epoch: 248, loss = 0.0031674359996831094\n",
      "Epoch: 249, loss = 0.0031656769814069814\n",
      "Epoch: 250, loss = 0.003163936138110871\n",
      "Epoch: 251, loss = 0.003162213164963861\n",
      "Epoch: 252, loss = 0.0031605077708343385\n",
      "Epoch: 253, loss = 0.003158819677360822\n",
      "Epoch: 254, loss = 0.003157148618090205\n",
      "Epoch: 255, loss = 0.003155494337678506\n",
      "Epoch: 256, loss = 0.0031538565911493946\n",
      "Epoch: 257, loss = 0.003152235143206166\n",
      "Epoch: 258, loss = 0.003150629767593273\n",
      "Epoch: 259, loss = 0.0031490402465036474\n",
      "Epoch: 260, loss = 0.0031474663700285484\n",
      "Epoch: 261, loss = 0.003145907935646652\n",
      "Epoch: 262, loss = 0.0031443647477496584\n",
      "Epoch: 263, loss = 0.0031428366172016127\n",
      "Epoch: 264, loss = 0.003141323360929531\n",
      "Epoch: 265, loss = 0.0031398248015430522\n",
      "Epoch: 266, loss = 0.0031383407669809656\n",
      "Epoch: 267, loss = 0.0031368710901826784\n",
      "Epoch: 268, loss = 0.0031354156087828313\n",
      "Epoch: 269, loss = 0.003133974164827384\n",
      "Epoch: 270, loss = 0.0031325466045095513\n",
      "Epoch: 271, loss = 0.003131132777924281\n",
      "Epoch: 272, loss = 0.0031297325388398465\n",
      "Epoch: 273, loss = 0.003128345744485343\n",
      "Epoch: 274, loss = 0.0031269722553529675\n",
      "Epoch: 275, loss = 0.0031256119350140305\n",
      "Epoch: 276, loss = 0.0031242646499476927\n",
      "Epoch: 277, loss = 0.0031229302693815066\n",
      "Epoch: 278, loss = 0.0031216086651430002\n",
      "Epoch: 279, loss = 0.0031202997115214247\n",
      "Epoch: 280, loss = 0.0031190032851390135\n",
      "Epoch: 281, loss = 0.0031177192648310197\n",
      "Epoch: 282, loss = 0.003116447531534005\n",
      "Epoch: 283, loss = 0.0031151879681816656\n",
      "Epoch: 284, loss = 0.003113940459607806\n",
      "Epoch: 285, loss = 0.003112704892455837\n",
      "Epoch: 286, loss = 0.0031114811550944227\n",
      "Epoch: 287, loss = 0.003110269137538813\n",
      "Epoch: 288, loss = 0.0031090687313774425\n",
      "Epoch: 289, loss = 0.003107879829703479\n",
      "Epoch: 290, loss = 0.0031067023270509395\n",
      "Epoch: 291, loss = 0.0031055361193350374\n",
      "Epoch: 292, loss = 0.0031043811037965428\n",
      "Epoch: 293, loss = 0.003103237178949779\n",
      "Epoch: 294, loss = 0.0031021042445340753\n",
      "Epoch: 295, loss = 0.0031009822014684014\n",
      "Epoch: 296, loss = 0.0030998709518089843\n",
      "Epoch: 297, loss = 0.0030987703987096576\n",
      "Epoch: 298, loss = 0.0030976804463848425\n",
      "Epoch: 299, loss = 0.0030966010000748534\n",
      "Epoch: 300, loss = 0.00309553196601349\n",
      "Epoch: 301, loss = 0.0030944732513976917\n",
      "Epoch: 302, loss = 0.003093424764359098\n",
      "Epoch: 303, loss = 0.003092386413937459\n",
      "Epoch: 304, loss = 0.003091358110055661\n",
      "Epoch: 305, loss = 0.0030903397634963886\n",
      "Epoch: 306, loss = 0.0030893312858801623\n",
      "Epoch: 307, loss = 0.003088332589644784\n",
      "Epoch: 308, loss = 0.00308734358802604\n",
      "Epoch: 309, loss = 0.0030863641950395495\n",
      "Epoch: 310, loss = 0.0030853943254637284\n",
      "Epoch: 311, loss = 0.0030844338948237816\n",
      "Epoch: 312, loss = 0.0030834828193766167\n",
      "Epoch: 313, loss = 0.0030825410160966746\n",
      "Epoch: 314, loss = 0.0030816084026625513\n",
      "Epoch: 315, loss = 0.003080684897444427\n",
      "Epoch: 316, loss = 0.003079770419492165\n",
      "Epoch: 317, loss = 0.0030788648885241177\n",
      "Epoch: 318, loss = 0.0030779682249165254\n",
      "Epoch: 319, loss = 0.0030770803496935075\n",
      "Epoch: 320, loss = 0.0030762011845175924\n",
      "Epoch: 321, loss = 0.0030753306516807374\n",
      "Epoch: 322, loss = 0.0030744686740958207\n",
      "Epoch: 323, loss = 0.003073615175288585\n",
      "Epoch: 324, loss = 0.0030727700793899636\n",
      "Epoch: 325, loss = 0.003071933311128799\n",
      "Epoch: 326, loss = 0.003071104795824895\n",
      "Epoch: 327, loss = 0.003070284459382418\n",
      "Epoch: 328, loss = 0.0030694722282836036\n",
      "Epoch: 329, loss = 0.0030686680295827153\n",
      "Epoch: 330, loss = 0.0030678717909002986\n",
      "Epoch: 331, loss = 0.003067083440417676\n",
      "Epoch: 332, loss = 0.0030663029068716675\n",
      "Epoch: 333, loss = 0.003065530119549508\n",
      "Epoch: 334, loss = 0.003064765008283998\n",
      "Epoch: 335, loss = 0.0030640075034487906\n",
      "Epoch: 336, loss = 0.003063257535953905\n",
      "Epoch: 337, loss = 0.0030625150372413663\n",
      "Epoch: 338, loss = 0.003061779939280978\n",
      "Epoch: 339, loss = 0.00306105217456627\n",
      "Epoch: 340, loss = 0.003060331676110574\n",
      "Epoch: 341, loss = 0.0030596183774431606\n",
      "Epoch: 342, loss = 0.003058912212605555\n",
      "Epoch: 343, loss = 0.0030582131161479326\n",
      "Epoch: 344, loss = 0.003057521023125584\n",
      "Epoch: 345, loss = 0.003056835869095517\n",
      "Epoch: 346, loss = 0.0030561575901130694\n",
      "Epoch: 347, loss = 0.0030554861227286957\n",
      "Epoch: 348, loss = 0.0030548214039847364\n",
      "Epoch: 349, loss = 0.0030541633714122897\n",
      "Epoch: 350, loss = 0.0030535119630281478\n",
      "Epoch: 351, loss = 0.0030528671173317796\n",
      "Epoch: 352, loss = 0.0030522287733023685\n",
      "Epoch: 353, loss = 0.0030515968703958914\n",
      "Epoch: 354, loss = 0.0030509713485422584\n",
      "Epoch: 355, loss = 0.00305035214814247\n",
      "Epoch: 356, loss = 0.003049739210065849\n",
      "Epoch: 357, loss = 0.0030491324756472624\n",
      "Epoch: 358, loss = 0.0030485318866843953\n",
      "Epoch: 359, loss = 0.0030479373854350815\n",
      "Epoch: 360, loss = 0.0030473489146146068\n",
      "Epoch: 361, loss = 0.0030467664173931073\n",
      "Epoch: 362, loss = 0.0030461898373929015\n",
      "Epoch: 363, loss = 0.003045619118685944\n",
      "Epoch: 364, loss = 0.0030450542057912167\n",
      "Epoch: 365, loss = 0.003044495043672189\n",
      "Epoch: 366, loss = 0.0030439415777342637\n",
      "Epoch: 367, loss = 0.0030433937538222712\n",
      "Epoch: 368, loss = 0.0030428515182179413\n",
      "Epoch: 369, loss = 0.0030423148176374126\n",
      "Epoch: 370, loss = 0.0030417835992287514\n",
      "Epoch: 371, loss = 0.003041257810569471\n",
      "Epoch: 372, loss = 0.0030407373996640752\n",
      "Epoch: 373, loss = 0.003040222314941603\n",
      "Epoch: 374, loss = 0.0030397125052531704\n",
      "Epoch: 375, loss = 0.0030392079198695563\n",
      "Epoch: 376, loss = 0.0030387085084787576\n",
      "Epoch: 377, loss = 0.003038214221183565\n",
      "Epoch: 378, loss = 0.003037725008499158\n",
      "Epoch: 379, loss = 0.003037240821350675\n",
      "Epoch: 380, loss = 0.0030367616110708326\n",
      "Epoch: 381, loss = 0.003036287329397514\n",
      "Epoch: 382, loss = 0.003035817928471368\n",
      "Epoch: 383, loss = 0.003035353360833435\n",
      "Epoch: 384, loss = 0.0030348935794227354\n",
      "Epoch: 385, loss = 0.0030344385375739116\n",
      "Epoch: 386, loss = 0.003033988189014848\n",
      "Epoch: 387, loss = 0.0030335424878642782\n",
      "Epoch: 388, loss = 0.0030331013886294336\n",
      "Epoch: 389, loss = 0.0030326648462036658\n",
      "Epoch: 390, loss = 0.003032232815864082\n",
      "Epoch: 391, loss = 0.003031805253269186\n",
      "Epoch: 392, loss = 0.003031382114456536\n",
      "Epoch: 393, loss = 0.0030309633558403613\n",
      "Epoch: 394, loss = 0.0030305489342092253\n",
      "Epoch: 395, loss = 0.003030138806723692\n",
      "Epoch: 396, loss = 0.0030297329309139473\n",
      "Epoch: 397, loss = 0.0030293312646774958\n",
      "Epoch: 398, loss = 0.0030289337662768017\n",
      "Epoch: 399, loss = 0.0030285403943369443\n",
      "Epoch: 400, loss = 0.0030281511078433126\n",
      "Epoch: 401, loss = 0.0030277658661392475\n",
      "Epoch: 402, loss = 0.003027384628923748\n",
      "Epoch: 403, loss = 0.00302700735624913\n",
      "Epoch: 404, loss = 0.0030266340085187025\n",
      "Epoch: 405, loss = 0.003026264546484483\n",
      "Epoch: 406, loss = 0.0030258989312448494\n",
      "Epoch: 407, loss = 0.00302553712424227\n",
      "Epoch: 408, loss = 0.003025179087260975\n",
      "Epoch: 409, loss = 0.0030248247824246786\n",
      "Epoch: 410, loss = 0.0030244741721942676\n",
      "Epoch: 411, loss = 0.003024127219365526\n",
      "Epoch: 412, loss = 0.003023783887066853\n",
      "Epoch: 413, loss = 0.0030234441387569657\n",
      "Epoch: 414, loss = 0.0030231079382226377\n",
      "Epoch: 415, loss = 0.0030227752495764326\n",
      "Epoch: 416, loss = 0.003022446037254427\n",
      "Epoch: 417, loss = 0.0030221202660139653\n",
      "Epoch: 418, loss = 0.0030217979009313943\n",
      "Epoch: 419, loss = 0.0030214789073998117\n",
      "Epoch: 420, loss = 0.003021163251126853\n",
      "Epoch: 421, loss = 0.0030208508981324016\n",
      "Epoch: 422, loss = 0.003020541814746403\n",
      "Epoch: 423, loss = 0.0030202359676066255\n",
      "Epoch: 424, loss = 0.00301993332365643\n",
      "Epoch: 425, loss = 0.0030196338501425717\n",
      "Epoch: 426, loss = 0.0030193375146129998\n",
      "Epoch: 427, loss = 0.0030190442849146243\n",
      "Epoch: 428, loss = 0.003018754129191162\n",
      "Epoch: 429, loss = 0.00301846701588093\n",
      "Epoch: 430, loss = 0.0030181829137146616\n",
      "Epoch: 431, loss = 0.0030179017917133506\n",
      "Epoch: 432, loss = 0.003017623619186077\n",
      "Epoch: 433, loss = 0.003017348365727854\n",
      "Epoch: 434, loss = 0.003017076001217471\n",
      "Epoch: 435, loss = 0.0030168064958153457\n",
      "Epoch: 436, loss = 0.003016539819961416\n",
      "Epoch: 437, loss = 0.00301627594437298\n",
      "Epoch: 438, loss = 0.003016014840042596\n",
      "Epoch: 439, loss = 0.003015756478235969\n",
      "Epoch: 440, loss = 0.003015500830489848\n",
      "Epoch: 441, loss = 0.0030152478686099188\n",
      "Epoch: 442, loss = 0.0030149975646687347\n",
      "Epoch: 443, loss = 0.003014749891003635\n",
      "Epoch: 444, loss = 0.0030145048202146582\n",
      "Epoch: 445, loss = 0.0030142623251625173\n",
      "Epoch: 446, loss = 0.003014022378966505\n",
      "Epoch: 447, loss = 0.003013784955002486\n",
      "Epoch: 448, loss = 0.0030135500269008286\n",
      "Epoch: 449, loss = 0.0030133175685444265\n",
      "Epoch: 450, loss = 0.003013087554066631\n",
      "Epoch: 451, loss = 0.003012859957849276\n",
      "Epoch: 452, loss = 0.0030126347545206796\n",
      "Epoch: 453, loss = 0.003012411918953638\n",
      "Epoch: 454, loss = 0.0030121914262634646\n",
      "Epoch: 455, loss = 0.003011973251806006\n",
      "Epoch: 456, loss = 0.0030117573711756986\n",
      "Epoch: 457, loss = 0.003011543760203601\n",
      "Epoch: 458, loss = 0.0030113323949554683\n",
      "Epoch: 459, loss = 0.003011123251729812\n",
      "Epoch: 460, loss = 0.0030109163070559785\n",
      "Epoch: 461, loss = 0.0030107115376922515\n",
      "Epoch: 462, loss = 0.003010508920623925\n",
      "Epoch: 463, loss = 0.0030103084330614406\n",
      "Epoch: 464, loss = 0.0030101100524384987\n",
      "Epoch: 465, loss = 0.0030099137564101675\n",
      "Epoch: 466, loss = 0.0030097195228510547\n",
      "Epoch: 467, loss = 0.003009527329853423\n",
      "Epoch: 468, loss = 0.003009337155725379\n",
      "Epoch: 469, loss = 0.003009148978989024\n",
      "Epoch: 470, loss = 0.003008962778378652\n",
      "Epoch: 471, loss = 0.003008778532838913\n",
      "Epoch: 472, loss = 0.003008596221523049\n",
      "Epoch: 473, loss = 0.0030084158237910757\n",
      "Epoch: 474, loss = 0.003008237319208014\n",
      "Epoch: 475, loss = 0.00300806068754213\n",
      "Epoch: 476, loss = 0.003007885908763171\n",
      "Epoch: 477, loss = 0.0030077129630406204\n",
      "Epoch: 478, loss = 0.003007541830741957\n",
      "Epoch: 479, loss = 0.00300737249243094\n",
      "Epoch: 480, loss = 0.003007204928865888\n",
      "Epoch: 481, loss = 0.003007039120997967\n",
      "Epoch: 482, loss = 0.003006875049969504\n",
      "Epoch: 483, loss = 0.0030067126971123183\n",
      "Epoch: 484, loss = 0.0030065520439460187\n",
      "Epoch: 485, loss = 0.003006393072176372\n",
      "Epoch: 486, loss = 0.003006235763693631\n",
      "Epoch: 487, loss = 0.0030060801005709\n",
      "Epoch: 488, loss = 0.003005926065062511\n",
      "Epoch: 489, loss = 0.0030057736396024008\n",
      "Epoch: 490, loss = 0.003005622806802498\n",
      "Epoch: 491, loss = 0.0030054735494511294\n",
      "Epoch: 492, loss = 0.0030053258505114444\n",
      "Epoch: 493, loss = 0.003005179693119809\n",
      "Epoch: 494, loss = 0.0030050350605842725\n",
      "Epoch: 495, loss = 0.0030048919363829933\n",
      "Epoch: 496, loss = 0.003004750304162706\n",
      "Epoch: 497, loss = 0.0030046101477371715\n",
      "Epoch: 498, loss = 0.0030044714510856726\n",
      "Epoch: 499, loss = 0.0030043341983514876\n",
      "Epoch: 500, loss = 0.003004198373840404\n"
     ]
    }
   ],
   "source": [
    "# Initialising weights and biases\n",
    "# Weights\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "\n",
    "# Biases\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# save the original weights and biases for comparison in the end\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# input values and target value\n",
    "#input1 = 1\n",
    "#input2 = 0\n",
    "#true_value = 2\n",
    "\n",
    "# learning rate\n",
    "LR = 0.0055\n",
    "epochs = 500\n",
    "\n",
    "# Using the previously defined function to generate data instead\n",
    "data = generate_test_data()\n",
    "\n",
    "# Let's scale our values with min/max -scaling\n",
    "data=(data - np.min(data))/(np.max(data)-np.min(data))\n",
    "\n",
    "# Points for plotting loss later\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "        # Forward pass\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        \n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        predicted_value = node_3_output\n",
    "        # This will probably crash if the loss value gets too high\n",
    "        # replace with Numpy64 if needed\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # Back propagation - last layer\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # Back propagation - next layer\n",
    "        # From this point the chain rule is needed\n",
    "\n",
    "        # Weights 1 to 4:\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # Biases 1 and 2:\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_part_deriv(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_part_deriv(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # Updating the weights and biases\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    # after each epoch, print the current weights and biases\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss = {average_loss}\")\n",
    "\n",
    "    \"\"\"print(f\"W1: {new_w1}\")\n",
    "    print(f\"W2: {new_w2}\")\n",
    "    print(f\"W3: {new_w3}\")\n",
    "    print(f\"W4: {new_w4}\")\n",
    "    print(f\"W5: {new_w5}\")\n",
    "    print(f\"W6: {new_w6}\")\n",
    "    print(f\"B1: {new_b1}\")\n",
    "    print(f\"B2: {new_b2}\")\n",
    "    print(f\"B3: {new_b3}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Original weights/biases\n",
      "W1: 1\n",
      "W2: 0.5\n",
      "W3: 1\n",
      "W4: -0.5\n",
      "W5: 1\n",
      "W6: 1\n",
      "B1: 0.5\n",
      "B2: 0\n",
      "B3: 0.5\n",
      "-------------------\n",
      "Final weights/biases\n",
      "W1: 1.696005314495832\n",
      "W2: 2.13700300532797\n",
      "W3: 0.7245258317764475\n",
      "W4: 0.0269341906702055\n",
      "W5: 1.30428725209625\n",
      "W6: 2.1818549969646344\n",
      "B1: 0.030994930037807376\n",
      "B2: -0.19289901797576944\n",
      "B3: 0.007864906278702073\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Original weights/biases\")\n",
    "print(f\"W1: {original_w1}\")\n",
    "print(f\"W2: {original_w2}\")\n",
    "print(f\"W3: {original_w3}\")\n",
    "print(f\"W4: {original_w4}\")\n",
    "print(f\"W5: {original_w5}\")\n",
    "print(f\"W6: {original_w6}\")\n",
    "print(f\"B1: {original_b1}\")\n",
    "print(f\"B2: {original_b2}\")\n",
    "print(f\"B3: {original_b3}\")\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Final weights/biases\")\n",
    "print(f\"W1: {w1}\")\n",
    "print(f\"W2: {w2}\")\n",
    "print(f\"W3: {w3}\")\n",
    "print(f\"W4: {w4}\")\n",
    "print(f\"W5: {w5}\")\n",
    "print(f\"W6: {w6}\")\n",
    "print(f\"B1: {bias1}\")\n",
    "print(f\"B2: {bias2}\")\n",
    "print(f\"B3: {bias3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction is just doing the forward pass step\n",
    "def predict(x1, x2):\n",
    "    node_1_output = x1 * w1 + x2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "    \n",
    "    node_2_output = x1 * w2 + x2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.505878613717743"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
